import warnings
from datetime import datetime
from pathlib import Path

import FinanceDataReader as fdr
import numpy as np
import pandas as pd
from tqdm import tqdm
from tqdm.auto import tqdm as tqdm_auto

tqdm_auto.pandas()
warnings.filterwarnings("ignore")

# Module-level constants
SCRIPT_DIR = Path(__file__).parent.resolve()
DATA_DIR = SCRIPT_DIR / "data"
STOCK_LIST_FILE = SCRIPT_DIR / "stock_list.csv"
FINANCIALS_FILE = SCRIPT_DIR / "financials.xlsx"
DELIST_REPORT_FILE = SCRIPT_DIR / "stock_delist.md"

# Technical analysis constants
SQRT_252 = np.sqrt(252)
ALPHA_14 = 1 / 14
EPSILON = 1e-10

# Analysis windows
RETURN_WINDOWS = [1, 5, 20, 30, 50, 60, 100, 120, 200]
DISPARITY_WINDOWS = [5, 20, 60, 120]

# Delisting detection threshold (days)
DELISTING_THRESHOLD_DAYS = 10


def data_download(start_date="2015-01-01", end_date="2024-12-31"):
    print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {SCRIPT_DIR}")
    print("íŒŒì¼ ìƒíƒœ:")
    print(f"  âœ“ stock_list.csv: {STOCK_LIST_FILE.exists()}")
    print(f"  âœ“ financials.xlsx: {FINANCIALS_FILE.exists()}")

    if not STOCK_LIST_FILE.exists():
        raise FileNotFoundError(f"stock_list.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {STOCK_LIST_FILE}")
    if not FINANCIALS_FILE.exists():
        raise FileNotFoundError(f"financials.xlsxë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FINANCIALS_FILE}")

    ticker_df = pd.read_csv(STOCK_LIST_FILE, encoding="cp949")
    tickers = ticker_df.iloc[:, 0].astype(str).str.zfill(6).tolist()

    ticker_to_name = dict(
        zip(ticker_df.iloc[:, 0].astype(str).str.zfill(6), ticker_df.iloc[:, 1])
    )

    print(f"\nâœ“ {len(ticker_df)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ")
    print(f"âœ“ {len(tickers)}ê°œ í‹°ì»¤ ì¶”ì¶œ ì™„ë£Œ")
    print(f"ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"ìˆ˜ì§‘í•  ì¢…ëª© ìˆ˜: {len(tickers)}\n")

    all_stocks = []
    failed_tickers = []
    delisted_info = []

    print("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...\n")

    for ticker in tqdm(tickers, desc="ì£¼ì‹ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘"):
        try:
            stock_df = fdr.DataReader(ticker, start_date, end_date)

            if not stock_df.empty:
                stock_df["Ticker"] = ticker
                stock_df = stock_df.reset_index()
                all_stocks.append(stock_df)
            else:
                failed_tickers.append(ticker)
                stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")
                delisted_info.append(
                    {
                        "ì¢…ëª©ì½”ë“œ": ticker,
                        "ì¢…ëª©ëª…": stock_name,
                        "ìƒíƒœ": "ë°ì´í„°ì—†ìŒ",
                        "ë§ˆì§€ë§‰ê±°ë˜ì¼": "N/A",
                        "ì‚¬ìœ ": "ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                    }
                )

        except Exception as e:
            failed_tickers.append(ticker)
            stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")
            delisted_info.append(
                {
                    "ì¢…ëª©ì½”ë“œ": ticker,
                    "ì¢…ëª©ëª…": stock_name,
                    "ìƒíƒœ": "ì˜¤ë¥˜ë°œìƒ",
                    "ë§ˆì§€ë§‰ê±°ë˜ì¼": "N/A",
                    "ì‚¬ìœ ": str(e)[:100],
                }
            )
            print(f"\n{ticker} ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {str(e)[:100]}")

    print(f"\nâœ“ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ: {len(all_stocks)}ê°œ ì¢…ëª©")
    if failed_tickers:
        print(f"âœ— ì‹¤íŒ¨: {len(failed_tickers)}ê°œ ì¢…ëª©")

    if not all_stocks:
        raise ValueError("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    df_all = pd.concat(all_stocks, ignore_index=True)
    df_all = df_all.sort_values(["Ticker", "Date"]).reset_index(drop=True)

    print(f"ê²°í•©ëœ DataFrame í¬ê¸°: {df_all.shape}")
    print(f"ë‚ ì§œ ë²”ìœ„: {df_all['Date'].min()} ~ {df_all['Date'].max()}")
    print(f"ê³ ìœ  ì¢…ëª© ìˆ˜: {df_all['Ticker'].nunique()}\n")

    print("ì¢…ëª©ëª… ì¶”ê°€ ë° ì¹´í…Œê³ ë¦¬ íƒ€ì… ë³€í™˜ ì¤‘...\n")

    df_all["ì¢…ëª©ëª…"] = df_all["Ticker"].map(ticker_to_name).astype("category")

    cols = df_all.columns.tolist()
    cols.remove("ì¢…ëª©ëª…")
    cols.insert(1, "ì¢…ëª©ëª…")
    df_all = df_all[cols]

    print("âœ“ ì¢…ëª©ëª…ì„ ì¹´í…Œê³ ë¦¬ íƒ€ì…ìœ¼ë¡œ ì¶”ê°€")
    print(f"DataFrame í¬ê¸°: {df_all.shape}\n")

    print("ëª¨ë“  ê¸°ìˆ ì  íŠ¹ì„± ê³„ì‚° ì¤‘ (ìˆ˜ìµë¥ , ê±°ë˜ëŸ‰, ì§€í‘œ, ì´ê²©ë„, ë¦¬ìŠ¤í¬)...\n")

    def calculate_all_features(group):
        for window in RETURN_WINDOWS:
            group[f"Return_{window}d"] = group["Close"].pct_change(periods=window) * 100

        group["vol_20"] = (
            group["Close"].pct_change().rolling(window=20, min_periods=20).std()
        )
        group["vol_60"] = (
            group["Close"].pct_change().rolling(window=60, min_periods=60).std()
        )
        group["vol_60_sqrt252"] = group["vol_60"] * SQRT_252
        group["log_vol"] = np.log(group["Volume"] + 1)
        log_vol_mean_60 = group["log_vol"].rolling(window=60, min_periods=60).mean()
        group["vol_ratio_60"] = group["log_vol"] - log_vol_mean_60
        group["avg_log_vol_ratio_60"] = (
            group["vol_ratio_60"].rolling(window=60, min_periods=60).mean()
        )
        group["std_log_vol_ratio_60"] = (
            group["vol_ratio_60"].rolling(window=60, min_periods=60).std()
        )

        delta = group["Close"].diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        avg_gain = gain.ewm(alpha=ALPHA_14, adjust=False, min_periods=14).mean()
        avg_loss = loss.ewm(alpha=ALPHA_14, adjust=False, min_periods=14).mean()
        rs = avg_gain / (avg_loss + EPSILON)
        group["RSI_14"] = 100 - (100 / (1 + rs))
        group["RSI_14_60avg"] = (
            group["RSI_14"].rolling(window=60, min_periods=60).mean()
        )

        typical_price = (group["High"] + group["Low"] + group["Close"]) / 3
        money_flow = typical_price * group["Volume"]
        positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0)
        negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0)
        positive_mf = positive_flow.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        negative_mf = negative_flow.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        mfi_ratio = positive_mf / (negative_mf + EPSILON)
        group["MFI_14"] = 100 - (100 / (1 + mfi_ratio))

        high_low = group["High"] - group["Low"]
        high_close = np.abs(group["High"] - group["Close"].shift(1))
        low_close = np.abs(group["Low"] - group["Close"].shift(1))
        true_range = pd.Series(
            np.maximum.reduce([high_low, high_close, low_close]), index=group.index
        )
        group["ATR_14"] = true_range.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        group["NATR_14"] = (group["ATR_14"] / group["Close"]) * 100

        high_diff = group["High"].diff()
        low_diff = -group["Low"].diff()
        plus_dm = high_diff.where((high_diff > low_diff) & (high_diff > 0), 0)
        minus_dm = low_diff.where((low_diff > high_diff) & (low_diff > 0), 0)
        smoothed_tr = true_range.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        smoothed_plus_dm = plus_dm.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        smoothed_minus_dm = minus_dm.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        plus_di = 100 * (smoothed_plus_dm / (smoothed_tr + EPSILON))
        minus_di = 100 * (smoothed_minus_dm / (smoothed_tr + EPSILON))
        dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + EPSILON)
        group["ADX_14"] = dx.ewm(alpha=ALPHA_14, adjust=False, min_periods=14).mean()

        for window in DISPARITY_WINDOWS:
            ma = group["Close"].rolling(window=window, min_periods=window).mean()
            group[f"Disparity_{window}d"] = ((group["Close"] - ma) / ma) * 100

        daily_returns = group["Close"].pct_change()
        group["Mean_60d"] = daily_returns.rolling(window=60, min_periods=60).mean()
        group["Median_60d"] = daily_returns.rolling(window=60, min_periods=60).median()
        group["Std_60d"] = daily_returns.rolling(window=60, min_periods=60).std()
        group["Sharpe_60d"] = group["Mean_60d"] / (group["Std_60d"] + EPSILON)

        mean_252 = daily_returns.rolling(window=252, min_periods=252).mean()
        std_252 = daily_returns.rolling(window=252, min_periods=252).std()
        group["Sharpe_252d"] = mean_252 / (std_252 + EPSILON)

        downside_returns = daily_returns.where(daily_returns < 0, 0)
        downside_std_60 = downside_returns.rolling(window=60, min_periods=60).std()
        group["Sortino_60d"] = group["Mean_60d"] / (downside_std_60 + EPSILON)

        downside_std_252 = downside_returns.rolling(window=252, min_periods=252).std()
        group["Sortino_252d"] = mean_252 / (downside_std_252 + EPSILON)

        group["Skewness_60d"] = daily_returns.rolling(window=60, min_periods=60).skew()
        group["Zscore_60d"] = (
            group["Close"] - group["Close"].rolling(window=60, min_periods=60).mean()
        ) / (group["Close"].rolling(window=60, min_periods=60).std() + EPSILON)

        risk_cols = [
            "Mean_60d",
            "Median_60d",
            "Std_60d",
            "Sharpe_60d",
            "Sharpe_252d",
            "Sortino_60d",
            "Sortino_252d",
            "Skewness_60d",
            "Zscore_60d",
        ]
        group[risk_cols] = group[risk_cols].replace([np.inf, -np.inf], np.nan)

        return group

    try:
        df_all = df_all.groupby("Ticker", group_keys=False).progress_apply(
            calculate_all_features
        )
    except AttributeError:
        df_all = df_all.groupby("Ticker", group_keys=False).apply(
            calculate_all_features
        )

    print("\nâœ“ í•˜ë‚˜ì˜ groupby íŒ¨ìŠ¤ë¡œ ëª¨ë“  ê¸°ìˆ ì  íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ!")
    print("  - ìˆ˜ìµë¥ : 9ê°œ ì»¬ëŸ¼")
    print("  - ê±°ë˜ëŸ‰: 7ê°œ ì»¬ëŸ¼")
    print("  - ì§€í‘œ: 6ê°œ ì»¬ëŸ¼ (RSI, MFI, ATR, NATR, ADX)")
    print("  - ì´ê²©ë„: 4ê°œ ì»¬ëŸ¼")
    print("  - ë¦¬ìŠ¤í¬: 9ê°œ ì»¬ëŸ¼")
    print(f"\nDataFrame í¬ê¸°: {df_all.shape}")
    print(f"ì´ ì»¬ëŸ¼ ìˆ˜: {len(df_all.columns)}\n")

    print("ì¬ë¬´ì œí‘œ ë°ì´í„°ë¥¼ Excelì—ì„œ ë¡œë“œ ì¤‘...\n")

    excel_file_obj = pd.ExcelFile(FINANCIALS_FILE)

    def load_financial_sheet_filtered(excel_obj, sheet_name, tickers_to_keep):
        df_raw = excel_obj.parse(sheet_name, header=None)

        all_tickers = df_raw.iloc[0, 1:].astype(str).tolist()
        company_names = df_raw.iloc[1, 1:].astype(str).tolist()
        dates = pd.to_datetime(df_raw.iloc[3:, 0])

        all_tickers_clean = [str(col).lstrip("aA").zfill(6) for col in all_tickers]

        tickers_set = set(tickers_to_keep)
        cols_to_keep = [
            i + 1 for i, t in enumerate(all_tickers_clean) if t in tickers_set
        ]
        tickers_kept = [all_tickers_clean[i - 1] for i in cols_to_keep]

        values = df_raw.iloc[3:, cols_to_keep]

        df = pd.DataFrame(values.values, index=dates, columns=tickers_kept)
        df = df.apply(pd.to_numeric, errors="coerce")
        df = df.sort_index()

        sheet_ticker_to_name = {
            tickers_kept[i]: company_names[cols_to_keep[i] - 1]
            for i in range(len(tickers_kept))
        }

        return df, sheet_ticker_to_name

    def load_raw_sheet_metric_filtered(excel_obj, metric_name, tickers_to_keep):
        df_raw = excel_obj.parse("RAW", header=None)
        dates = pd.to_datetime(df_raw.iloc[0, 3:])
        metric_rows = df_raw[
            df_raw.iloc[:, 2].astype(str).str.contains(metric_name, na=False)
        ]

        tickers_set = set(tickers_to_keep)
        ticker_data = {}

        for idx, row in metric_rows.iterrows():
            ticker = str(row.iloc[0]).lstrip("aA").zfill(6)
            if ticker in tickers_set:
                values = row.iloc[3:].values
                ticker_series = pd.Series(values, index=dates)
                ticker_data[ticker] = ticker_series

        df_metric = pd.DataFrame(ticker_data)
        df_metric = df_metric.apply(pd.to_numeric, errors="coerce")
        df_metric = df_metric.sort_index()

        return df_metric

    print(f"Excel íŒŒì¼: {FINANCIALS_FILE}")

    print("BPS ì‹œíŠ¸ ë¡œë“œ ì¤‘...")
    df_bps, bps_names = load_financial_sheet_filtered(excel_file_obj, "BPS", tickers)
    print(f"  âœ“ BPS: {df_bps.shape[0]}ê°œ ë‚ ì§œ Ã— {df_bps.shape[1]}ê°œ í‹°ì»¤ (í•„í„°ë¨)")

    print("DPS ì‹œíŠ¸ ë¡œë“œ ì¤‘...")
    df_dps, dps_names = load_financial_sheet_filtered(excel_file_obj, "DPS", tickers)
    print(f"  âœ“ DPS: {df_dps.shape[0]}ê°œ ë‚ ì§œ Ã— {df_dps.shape[1]}ê°œ í‹°ì»¤ (í•„í„°ë¨)")

    print("EPS ì‹œíŠ¸ ë¡œë“œ ì¤‘...")
    df_eps, eps_names = load_financial_sheet_filtered(excel_file_obj, "EPS", tickers)
    print(f"  âœ“ EPS: {df_eps.shape[0]}ê°œ ë‚ ì§œ Ã— {df_eps.shape[1]}ê°œ í‹°ì»¤ (í•„í„°ë¨)")

    print("ë°°ë‹¹ìˆ˜ìµë¥  ì‹œíŠ¸ ë¡œë“œ ì¤‘...")
    df_div_yield, div_yield_names = load_financial_sheet_filtered(
        excel_file_obj, "ë°°ë‹¹ìˆ˜ìµë¥ ", tickers
    )
    print(
        f"  âœ“ ë°°ë‹¹ìˆ˜ìµë¥ : {df_div_yield.shape[0]}ê°œ ë‚ ì§œ Ã— {df_div_yield.shape[1]}ê°œ í‹°ì»¤ (í•„í„°ë¨)"
    )

    print("RAW ì‹œíŠ¸ì—ì„œ ë§¤ì¶œì•¡ ë¡œë“œ ì¤‘...")
    df_revenue = load_raw_sheet_metric_filtered(excel_file_obj, "ë§¤ì¶œì•¡", tickers)
    print(
        f"  âœ“ ë§¤ì¶œì•¡: {df_revenue.shape[0]}ê°œ ë‚ ì§œ Ã— {df_revenue.shape[1]}ê°œ í‹°ì»¤ (í•„í„°ë¨)"
    )

    print("RAW ì‹œíŠ¸ì—ì„œ ì˜ì—…ì´ìµ ë¡œë“œ ì¤‘...")
    df_op_profit = load_raw_sheet_metric_filtered(excel_file_obj, "ì˜ì—…ì´ìµ", tickers)
    print(
        f"  âœ“ ì˜ì—…ì´ìµ: {df_op_profit.shape[0]}ê°œ ë‚ ì§œ Ã— {df_op_profit.shape[1]}ê°œ í‹°ì»¤ (í•„í„°ë¨)"
    )

    print("\nâœ“ ì¬ë¬´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n")

    print("ì§€ì—° ì ìš© ì¤‘...\n")

    def apply_financial_lag(df_financial):
        df_lagged = df_financial.copy()
        df_lagged.index = df_lagged.index + pd.DateOffset(months=3)
        return df_lagged

    def apply_quarterly_lag(df_quarterly):
        df_lagged = df_quarterly.copy()

        new_index = []
        for date in df_lagged.index:
            month = date.month
            year = date.year

            if month == 3:
                new_date = pd.Timestamp(year=year, month=4, day=1)
            elif month == 6:
                new_date = pd.Timestamp(year=year, month=7, day=1)
            elif month == 9:
                new_date = pd.Timestamp(year=year, month=10, day=1)
            elif month == 12:
                new_date = pd.Timestamp(year=year + 1, month=1, day=1)
            else:
                new_date = date

            new_index.append(new_date)

        df_lagged.index = pd.DatetimeIndex(new_index)
        return df_lagged

    df_bps_lagged = apply_financial_lag(df_bps)
    df_dps_lagged = apply_financial_lag(df_dps)
    df_eps_lagged = apply_financial_lag(df_eps)
    df_div_yield_lagged = apply_financial_lag(df_div_yield)

    df_revenue_lagged = apply_quarterly_lag(df_revenue)
    df_op_profit_lagged = apply_quarterly_lag(df_op_profit)

    print("âœ“ ì§€ì—° ì ìš© ì™„ë£Œ:")
    print("  - ì¼ê°„ ë°ì´í„° (BPS, DPS, EPS, ë°°ë‹¹ìˆ˜ìµë¥ ): 3ê°œì›” ì§€ì—°")
    print("  - ë¶„ê¸° ë°ì´í„° (ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ): ë‹¤ìŒ ë¶„ê¸° ì‹œì‘ë¶€í„° ì‚¬ìš© ê°€ëŠ¥")
    print("    Â· Q1 (3ì›” 31ì¼) â†’ 4ì›” 1ì¼")
    print("    Â· Q2 (6ì›” 30ì¼) â†’ 7ì›” 1ì¼")
    print("    Â· Q3 (9ì›” 30ì¼) â†’ 10ì›” 1ì¼")
    print("    Â· Q4 (12ì›” 31ì¼) â†’ 1ì›” 1ì¼ (ë‹¤ìŒ í•´)\n")

    print("ì¬ë¬´ ë°ì´í„° ë³‘í•© ë° íŒŒìƒ ì§€í‘œ ê³„ì‚° ì¤‘...\n")

    def merge_financial_data(group):
        ticker = group.name

        group = group.reset_index()

        if "Date" not in group.columns and "index" in group.columns:
            group = group.rename(columns={"index": "Date"})

        group["Date"] = pd.to_datetime(group["Date"])
        group = group.sort_values("Date")

        mapping = {
            "BPS": df_bps_lagged,
            "DPS": df_dps_lagged,
            "EPS": df_eps_lagged,
            "ë°°ë‹¹ìˆ˜ìµë¥ ": df_div_yield_lagged,
            "ë§¤ì¶œì•¡": df_revenue_lagged,
            "ì˜ì—…ì´ìµ": df_op_profit_lagged,
        }

        for col_name, df_source in mapping.items():
            if ticker in df_source.columns:
                f_data = df_source[ticker].dropna().to_frame(name=col_name)
                f_data = f_data.reset_index()
                f_data.columns = ["Date", col_name]
                f_data["Date"] = pd.to_datetime(f_data["Date"])
                f_data = f_data.sort_values("Date")

                group = pd.merge_asof(group, f_data, on="Date", direction="backward")
            else:
                group[col_name] = np.nan

        group["PER"] = group["Close"] / group["EPS"]
        group["PBR"] = group["Close"] / group["BPS"]
        group["ROE"] = group["PBR"] / group["PER"]
        group["ë°°ë‹¹ì„±í–¥"] = (group["DPS"] / group["EPS"]) * 100

        group["ROE_YoY"] = (group["ROE"] / group["ROE"].shift(252) - 1) * 100
        group["EPS_YoY"] = (group["EPS"] / group["EPS"].shift(252) - 1) * 100
        group["ì˜ì—…ì´ìµ_YoY"] = (
            group["ì˜ì—…ì´ìµ"] / group["ì˜ì—…ì´ìµ"].shift(252) - 1
        ) * 100
        group["ë§¤ì¶œì•¡_YoY"] = (group["ë§¤ì¶œì•¡"] / group["ë§¤ì¶œì•¡"].shift(252) - 1) * 100

        metrics = [
            "PER",
            "PBR",
            "ROE",
            "ë°°ë‹¹ì„±í–¥",
            "ROE_YoY",
            "EPS_YoY",
            "ì˜ì—…ì´ìµ_YoY",
            "ë§¤ì¶œì•¡_YoY",
        ]
        group[metrics] = group[metrics].replace([np.inf, -np.inf], np.nan)

        group["Ticker"] = ticker

        return group.set_index(["Date", "Ticker"])

    try:
        df_all = df_all.groupby("Ticker", group_keys=False).progress_apply(
            merge_financial_data
        )
    except AttributeError:
        df_all = df_all.groupby("Ticker", group_keys=False).apply(
            merge_financial_data
        )

    print("âœ“ ì¬ë¬´ì œí‘œ ì»¬ëŸ¼ ì¶”ê°€:")
    print("  - BPS, DPS, EPS, ë°°ë‹¹ìˆ˜ìµë¥ , ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ")
    print("\nâœ“ íŒŒìƒ ì§€í‘œ ê³„ì‚°:")
    print("  - PER, PBR, ROE, ë°°ë‹¹ì„±í–¥")
    print("  - ROE_YoY, EPS_YoY, ì˜ì—…ì´ìµ_YoY, ë§¤ì¶œì•¡_YoY")
    print(f"\nìµœì¢… DataFrame í¬ê¸°: {df_all.shape}")
    print(f"ì´ ì»¬ëŸ¼ ìˆ˜: {len(df_all.columns)}\n")

    output_dir = DATA_DIR
    base_output_dir = output_dir
    counter = 1
    while output_dir.exists():
        output_dir = Path(f"{base_output_dir} ({counter})")
        counter += 1

    output_dir.mkdir(parents=True, exist_ok=True)

    all_tickers = df_all.index.get_level_values("Ticker").unique()

    print(f"ê°œë³„ ì¢…ëª© íŒŒì¼ ì €ì¥ ì¤‘: {output_dir}\n")

    stock_metadata = {}

    for ticker in tqdm(all_tickers, desc="parquet íŒŒì¼ ì €ì¥ ì¤‘"):
        stock_df = df_all.xs(ticker, level="Ticker").copy()

        cols_to_drop = ["Ticker", "index", "level_0"]
        stock_df = stock_df.drop(
            columns=[c for c in cols_to_drop if c in stock_df.columns]
        )

        stock_df = stock_df.sort_index()

        stock_metadata[ticker] = {
            "last_date": stock_df.index.max(),
            "last_close": stock_df.loc[stock_df.index.max(), "Close"],
        }

        stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")

        filename = f"{ticker}_{stock_name}.parquet"
        filepath = output_dir / filename

        stock_df.to_parquet(filepath, compression="snappy", index=True)

    unique_ticker_count = len(all_tickers)
    col_count = stock_df.shape[1] if not all_tickers.empty else 0

    print("\nìƒì¥íì§€ ì¢…ëª© ê²€ì‚¬ ì¤‘...\n")

    end_date_dt = pd.to_datetime(end_date)
    delisting_threshold = end_date_dt - pd.Timedelta(days=DELISTING_THRESHOLD_DAYS)

    for ticker, metadata in tqdm(stock_metadata.items(), desc="ìƒì¥íì§€ ê²€ì‚¬ ì¤‘"):
        last_date = metadata["last_date"]
        last_close = metadata["last_close"]

        stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")

        is_delisted = False
        delisting_reason = ""

        if pd.isna(last_close) or last_close == 0:
            is_delisted = True
            delisting_reason = "ì¢…ê°€ê°€ 0 ë˜ëŠ” NaN"
        elif last_date < delisting_threshold:
            is_delisted = True
            delisting_reason = (
                f"ê±°ë˜ ì¤‘ë‹¨ ({last_date.strftime('%Y-%m-%d')}ì— ë§ˆì§€ë§‰ ê±°ë˜)"
            )

        if is_delisted:
            delisted_info.append(
                {
                    "ì¢…ëª©ì½”ë“œ": ticker,
                    "ì¢…ëª©ëª…": stock_name,
                    "ìƒíƒœ": "ìƒì¥íì§€",
                    "ë§ˆì§€ë§‰ê±°ë˜ì¼": last_date.strftime("%Y-%m-%d"),
                    "ì‚¬ìœ ": delisting_reason,
                }
            )

    print(f"\n{'=' * 80}")
    print("ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'=' * 80}")
    print(f"âœ“ {unique_ticker_count}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    print(f"âœ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"âœ“ íŒŒì¼ë‹¹ ì»¬ëŸ¼ ìˆ˜: {col_count}ê°œ")
    print("\níŠ¹ì„± ë¶„ë¥˜:")
    print("  - OHLCV: 7ê°œ (ì¢…ëª©ëª…, Open, High, Low, Close, Volume, Change)")
    print("  - ìˆ˜ìµë¥ : 9ê°œ (1d, 5d, 20d, 30d, 50d, 60d, 100d, 120d, 200d)")
    print(
        "  - ê±°ë˜ëŸ‰: 7ê°œ (vol_20, vol_60, vol_60_sqrt252, log_vol, vol_ratio_60, avg_log_vol_ratio_60, std_log_vol_ratio_60)"
    )
    print("  - ê¸°ìˆ ì§€í‘œ: 6ê°œ (RSI_14, RSI_14_60avg, MFI_14, ATR_14, NATR_14, ADX_14)")
    print("  - ì´ê²©ë„: 4ê°œ (5d, 20d, 60d, 120d)")
    print(
        "  - ë¦¬ìŠ¤í¬: 9ê°œ (Mean_60d, Median_60d, Std_60d, Sharpe_60d, Sharpe_252d, Sortino_60d, Sortino_252d, Skewness_60d, Zscore_60d)"
    )
    print("  - ì¬ë¬´ì œí‘œ: 6ê°œ (BPS, DPS, EPS, ë°°ë‹¹ìˆ˜ìµë¥ , ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ)")
    print(
        "  - íŒŒìƒì§€í‘œ: 8ê°œ (PER, PBR, ROE, ë°°ë‹¹ì„±í–¥, ROE_YoY, EPS_YoY, ì˜ì—…ì´ìµ_YoY, ë§¤ì¶œì•¡_YoY)"
    )
    print(f"  - ì´í•©: {col_count}ê°œ ì»¬ëŸ¼")
    print(f"{'=' * 80}\n")

    if delisted_info:
        delisted_md_path = DELIST_REPORT_FILE

        delisted_count = 0
        error_count = 0
        for d in delisted_info:
            if d["ìƒíƒœ"] == "ìƒì¥íì§€":
                delisted_count += 1
            else:
                error_count += 1
        active_count = unique_ticker_count - delisted_count

        with open(delisted_md_path, "w", encoding="utf-8") as f:
            f.write("# ìƒì¥íì§€ ë° ë°ì´í„° ì˜¤ë¥˜ ì¢…ëª© ëª©ë¡\n\n")
            f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\n\n")
            f.write("## ğŸ“Š ìš”ì•½\n\n")
            f.write(f"- ì´ ë¶„ì„ ì¢…ëª© ìˆ˜: {unique_ticker_count}ê°œ\n")
            f.write(f"- ì •ìƒ ê±°ë˜ ì¢…ëª©: {active_count}ê°œ\n")
            f.write(f"- ìƒì¥íì§€ ì¢…ëª©: {delisted_count}ê°œ\n")
            f.write(f"- ë°ì´í„° ì˜¤ë¥˜ ì¢…ëª©: {error_count}ê°œ\n\n")
            f.write("---\n\n")
            f.write("## ğŸ” ìƒì„¸ ëª©ë¡\n\n")
            f.write("| ì¢…ëª©ì½”ë“œ | ì¢…ëª©ëª… | ìƒíƒœ | ë§ˆì§€ë§‰ê±°ë˜ì¼ | ì‚¬ìœ  |\n")
            f.write("|---------|--------|------|-------------|------|\n")
            for info in delisted_info:
                f.write(
                    f"| {info['ì¢…ëª©ì½”ë“œ']} | {info['ì¢…ëª©ëª…']} | {info['ìƒíƒœ']} | {info['ë§ˆì§€ë§‰ê±°ë˜ì¼']} | {info['ì‚¬ìœ ']} |\n"
                )
            f.write("\n---\n\n")
            f.write("## ğŸ“ ì°¸ê³ ì‚¬í•­\n\n")
            f.write(
                f"- ì´ ë¦¬ìŠ¤íŠ¸ëŠ” {start_date}ë¶€í„° {end_date}ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            )
            f.write(
                f"- ë§ˆì§€ë§‰ ê±°ë˜ì¼ì´ {delisting_threshold.strftime('%Y-%m-%d')} ì´ì „ì¸ ì¢…ëª©ì€ ìƒì¥íì§€ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n"
            )
            f.write("- ì¢…ê°€ê°€ 0ì´ê±°ë‚˜ NaNì¸ ì¢…ëª©ì€ ìƒì¥íì§€ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n")
            f.write("- **ìƒì¥íì§€ ì¢…ëª©ì˜ ë°ì´í„°ëŠ” ë§ˆì§€ë§‰ ê±°ë˜ì¼ê¹Œì§€ ë³´ì¡´ë©ë‹ˆë‹¤.**\n")

        print(f"\nâœ“ ìƒì¥íì§€ ë° ì˜¤ë¥˜ ì¢…ëª© ì •ë³´ ì €ì¥: {delisted_md_path}")
        print(f"  - ì •ìƒ ê±°ë˜: {active_count}ê°œ")
        print(f"  - ìƒì¥íì§€: {delisted_count}ê°œ")
        print(f"  - ë°ì´í„° ì˜¤ë¥˜: {error_count}ê°œ\n")

    return df_all


def data_load():
    print(f"ğŸ“ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜: {SCRIPT_DIR}")
    print(f"ğŸ“‚ ë°ì´í„° í´ë” íƒìƒ‰ ì¤‘: {DATA_DIR.absolute()}")

    if not DATA_DIR.exists():
        print(
            f"\nâŒ ì˜¤ë¥˜: '{DATA_DIR.absolute()}' ìœ„ì¹˜ì—ì„œ 'data' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
        print(
            "ğŸ’¡ collect_create_data.py íŒŒì¼ê³¼ ê°™ì€ ìœ„ì¹˜ì— 'data' í´ë”ë¥¼ ë§Œë“¤ê³  .parquet íŒŒì¼ë“¤ì„ ë„£ì–´ì£¼ì„¸ìš”."
        )
        return None

    parquet_files = list(DATA_DIR.glob("*.parquet"))

    if not parquet_files:
        print("\nâŒ ì˜¤ë¥˜: 'data' í´ë”ëŠ” ì¡´ì¬í•˜ì§€ë§Œ, ë‚´ë¶€ì— .parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    print(f"\nâœ… ë°ì´í„° í´ë” ë°œê²¬: {DATA_DIR.absolute()}")
    print(f"âœ… {len(parquet_files)}ê°œì˜ parquet íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    all_stocks = []
    failed_files = []

    for file_path in tqdm(parquet_files, desc="parquet íŒŒì¼ ë¡œë“œ ì¤‘"):
        try:
            stock_df = pd.read_parquet(file_path)

            ticker_code = file_path.stem.split("_")[0]
            stock_df["Ticker"] = ticker_code

            all_stocks.append(stock_df)
        except Exception as e:
            failed_files.append((file_path.name, str(e)))

    if not all_stocks:
        print("\nâŒ ì½ì„ ìˆ˜ ìˆëŠ” parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    df_all = pd.concat(all_stocks, ignore_index=False)
    df_all.index = pd.to_datetime(df_all.index)
    df_all.index.name = "Date"
    df_all = df_all.set_index("Ticker", append=True)
    df_all = df_all.sort_index()

    print(f"\nâœ“ ë©€í‹°ì¸ë±ìŠ¤ DataFrame ìƒì„± ì™„ë£Œ (ì´ {len(df_all):,}í–‰ ë¡œë“œ)")

    if failed_files:
        print(f"\nâš ï¸  ì½ì§€ ëª»í•œ íŒŒì¼: {len(failed_files)}ê°œ")
        for filename, error in failed_files[:5]:
            print(f"  - {filename}: {error[:50]}")
        if len(failed_files) > 5:
            print(f"  ... ì™¸ {len(failed_files) - 5}ê°œ ë”")

    print(f"\n{'=' * 80}")
    print(f"ì¸ë±ìŠ¤: {df_all.index.names}")
    print(f"ì»¬ëŸ¼ ìˆ˜: {len(df_all.columns)}ê°œ")
    print(f"{'=' * 80}")
    print("\nì»¬ëŸ¼ ëª©ë¡:")
    for i, col in enumerate(df_all.columns, 1):
        print(f"{i}. {col}")
    print(f"{'=' * 80}\n")

    return df_all


def data_query(df):
    if df is None:
        print("âŒ ì˜¤ë¥˜: DataFrameì´ Noneì…ë‹ˆë‹¤. ë¨¼ì € data_load()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None

    idx = pd.IndexSlice

    print("\n" + "=" * 80)
    print("ë°ì´í„° ì¿¼ë¦¬")
    print("=" * 80)

    start_date_input = input("\nì‹œì‘ ë‚ ì§œ (YYYY-MM-DD) [ì—”í„°=ì „ì²´ ì‹œì‘ì¼]: ").strip()
    if start_date_input == "":
        start_date = df.index.get_level_values("Date").min()
        print(f"â†’ ì‹œì‘ ë‚ ì§œ: {start_date.strftime('%Y-%m-%d')} (ì „ì²´ ì‹œì‘ì¼)")
    else:
        start_date = start_date_input
        print(f"â†’ ì‹œì‘ ë‚ ì§œ: {start_date}")

    end_date_input = input("ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD) [ì—”í„°=ì „ì²´ ì¢…ë£Œì¼]: ").strip()
    if end_date_input == "":
        end_date = df.index.get_level_values("Date").max()
        print(f"â†’ ì¢…ë£Œ ë‚ ì§œ: {end_date.strftime('%Y-%m-%d')} (ì „ì²´ ì¢…ë£Œì¼)")
    else:
        end_date = end_date_input
        print(f"â†’ ì¢…ë£Œ ë‚ ì§œ: {end_date}")

    tickers_input = input(
        "\nì°¾ì„ í‹°ì»¤ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 005930,000660) [ì—”í„°=ì „ì²´]: "
    ).strip()
    if tickers_input == "":
        tickers_to_find = []
        print("â†’ í‹°ì»¤: ì „ì²´")
    else:
        tickers_to_find = [t.strip() for t in tickers_input.split(",")]
        print(f"â†’ í‹°ì»¤: {tickers_to_find}")

    columns_input = input(
        "ì°¾ì„ ì»¬ëŸ¼ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: Close,Volume,PER) [ì—”í„°=ì „ì²´]: "
    ).strip()
    if columns_input == "":
        columns_to_find = []
        print("â†’ ì»¬ëŸ¼: ì „ì²´")
    else:
        columns_to_find = [c.strip() for c in columns_input.split(",")]
        print(f"â†’ ì»¬ëŸ¼: {columns_to_find}")

    print("\n" + "=" * 80)
    print("ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
    print("=" * 80 + "\n")

    if tickers_to_find:
        df_final = df.loc[idx[start_date:end_date, tickers_to_find], :]
    else:
        df_final = df.loc[idx[start_date:end_date, :], :]

    if columns_to_find:
        available_cols = [c for c in columns_to_find if c in df_final.columns]
        missing_cols = [c for c in columns_to_find if c not in df_final.columns]

        if missing_cols:
            print(f"âš ï¸  ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼: {missing_cols}")

        if available_cols:
            df_final = df_final[available_cols]
        else:
            print("âŒ ì˜¤ë¥˜: ìœ íš¨í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return None

    print(f"\n{'=' * 80}")
    print(f"ê²€ìƒ‰ ê²°ê³¼ ({start_date} ~ {end_date})")
    print(f"{'=' * 80}")
    print(f"ë°œê²¬ëœ í‹°ì»¤: {df_final.index.get_level_values('Ticker').unique().tolist()}")
    print(f"ìœ ì§€ëœ ì»¬ëŸ¼: {df_final.columns.tolist()}")
    print(f"ì´ í–‰ ìˆ˜: {len(df_final):,}ê°œ")
    print(f"{'=' * 80}\n")

    print("ì²˜ìŒ 10í–‰:")
    print(df_final.head(10))
    print("\në§ˆì§€ë§‰ 10í–‰:")
    print(df_final.tail(10))

    return df_final


if __name__ == "__main__":
    print("=" * 80)
    print("ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜:")
    print("1. data_download(start_date='2015-01-01', end_date='2024-12-31')")
    print("   - stock_list.csvì—ì„œ ì¢…ëª©ì„ ì½ê³  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ data í´ë”ì— ì €ì¥")
    print("\n2. df = data_load()")
    print("   - data í´ë”ì˜ parquet íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ë©€í‹°ì¸ë±ìŠ¤ DataFrame ìƒì„±")
    print("\n3. result = data_query(df)")
    print("   - ë‚ ì§œ, í‹°ì»¤, ì»¬ëŸ¼ìœ¼ë¡œ ë°ì´í„° í•„í„°ë§")
    print("=" * 80 + "\n")
