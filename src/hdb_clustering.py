import os
import glob
import re
import traceback
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler, RobustScaler
from adjustText import adjust_text

# ì„¤ì • íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (DBSCAN ì „ìš© ì„¤ì •)
# import config
from config import (DATA_FOLDER,OUTPUT_FOLDER, X_FEATS,Y_FEATS)
from config import (TARGET_CLUSTERS_MIN,TARGET_CLUSTERS_MAX)
from config import (EPS_RANGE_START, EPS_RANGE_END, EPS_STEP, MIN_SAMPLES)
from config import (FONT_FAMILY, FIG_SIZE,CSV_ENCODING)


plt.rcParams["font.family"] = FONT_FAMILY
plt.rcParams["axes.unicode_minus"] = False

class DataProcessor:
    @staticmethod
    def _safe_float(val):
        try:
            if isinstance(val, (pd.Series, np.ndarray, list)):
                if len(val) == 0:
                    return 0.0
                return DataProcessor._safe_float(
                    val.iloc[0] if hasattr(val, "iloc") else val[0]
                )

            if isinstance(val, str):
                val = re.sub(r"[^0-9.\-]", "", val)
                if not val:
                    return 0.0

            v = float(val)
            if np.isnan(v) or np.isinf(v):
                return 0.0
            return v
        except:
            return 0.0

    @staticmethod
    def load_snapshot(target_date_str):
        target_date = pd.to_datetime(target_date_str)
        data_dir = os.path.abspath(DATA_FOLDER)
        files = glob.glob(os.path.join(data_dir, "*.parquet"))

        print(f"[INFO] ë°ì´í„° í´ë”: {data_dir}")
        print(f"[INFO] {len(files)}ê°œ íŒŒì¼ ë¡œë”© ì¤‘... ({target_date_str})")

        clean_rows = []

        for file_path in files:
            try:
                df = pd.read_parquet(file_path)
                df = df.loc[:, ~df.columns.duplicated()]

                if "Date" in df.columns:
                    df["Date"] = pd.to_datetime(df["Date"])
                    df = df.set_index("Date")

                if target_date not in df.index:
                    continue

                row = df.loc[target_date]
                if isinstance(row, pd.DataFrame):
                    row = row.iloc[-1]

                filename = os.path.basename(file_path)
                name_match = re.match(r"(\d{6})_(.+)\.parquet", filename)
                ticker = name_match.group(1) if name_match else "Unknown"
                name = (
                    name_match.group(2)
                    if name_match
                    else filename.replace(".parquet", "")
                )

                marcap = 0.0
                for mc in ["Marcap", "ìƒì¥ì‹œê°€ì´ì•¡", "ì‹œê°€ì´ì•¡"]:
                    if mc in row.index:
                        marcap = DataProcessor._safe_float(row[mc])
                        break

                x_val = 0.0
                for f in X_FEATS:
                    if f in row.index:
                        x_val = DataProcessor._safe_float(row[f])
                        break

                y_val = 0.0
                for f in Y_FEATS:
                    if f in row.index:
                        y_val = DataProcessor._safe_float(row[f])
                        break

                clean_rows.append(
                    {
                        "Ticker": ticker,
                        "Name": name,
                        "Marcap": marcap,
                        "X_Raw": x_val,
                        "Y_Raw": y_val,
                    }
                )
            except:
                continue

        return pd.DataFrame(clean_rows)


class AutoDBSCAN:
    def run(self, df):
        # 1. NaN ì œê±°
        df["X_Raw"] = df["X_Raw"].fillna(0.0)
        df["Y_Raw"] = df["Y_Raw"].fillna(0.0)

        # 2. ì´ìƒì¹˜ ì²˜ë¦¬ (Winsorizing: ìƒìœ„ 1% ê°’ì„ 99% ê°’ìœ¼ë¡œ ëŒ€ì²´)
        # ì´ë ‡ê²Œ í•˜ë©´ ê·¹ë‹¨ê°’ì´ ì‚¬ë¼ì ¸ì„œ ê·¸ë˜í”„ê°€ ì˜ˆì˜ê²Œ í´ì§
        p99_x = df["X_Raw"].quantile(0.99)
        p99_y = df["Y_Raw"].quantile(0.99)
        p01_y = df["Y_Raw"].quantile(0.01)

        df["X_Clipped"] = df["X_Raw"].clip(upper=p99_x)
        df["Y_Clipped"] = df["Y_Raw"].clip(lower=p01_y, upper=p99_y)

        # 3. ìŠ¤ì¼€ì¼ë§ (RobustScalerë¡œ ì¤‘ì•™ê°’ ì¤‘ì‹¬ ì •ë ¬)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(df[["X_Clipped", "Y_Clipped"]])

        # 4. DBSCAN ìë™ íŠœë‹
        best_labels = None
        best_eps = 0.5
        found = False

        print(
            f"[TUNING] ëª©í‘œ êµ°ì§‘: {TARGET_CLUSTERS_MIN}~{TARGET_CLUSTERS_MAX}ê°œ ì°¾ëŠ” ì¤‘..."
        )

        for eps in np.arange(
            EPS_RANGE_START, EPS_RANGE_END, EPS_STEP
        ):
            db = DBSCAN(eps=eps, min_samples=MIN_SAMPLES).fit(X_scaled)
            labels = db.labels_
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

            if TARGET_CLUSTERS_MIN <= n_clusters <= TARGET_CLUSTERS_MAX:
                best_labels = labels
                best_eps = eps
                found = True
                print(f" -> ì„±ê³µ: eps={eps:.2f}, êµ°ì§‘={n_clusters}ê°œ")
                break

            if 2 <= n_clusters <= 10:
                best_labels = labels
                best_eps = eps

        if not found and best_labels is None:
            print(" -> ê¸°ë³¸ê°’ ì‹¤í–‰")
            db = DBSCAN(eps=0.5, min_samples=MIN_SAMPLES).fit(X_scaled)
            best_labels = db.labels_
            best_eps = 0.5
        elif not found:
            print(f" -> ì°¨ì„ ì±… ì‹¤í–‰ (eps={best_eps:.2f})")

        df["Cluster"] = best_labels

        # 5. ë¼ë²¨ë§
        cluster_info = []
        unique_labels = sorted(list(set(best_labels) - {-1}))

        med_x = df["X_Clipped"].median()
        med_y = df["Y_Clipped"].median()

        for c in unique_labels:
            group = df[df["Cluster"] == c]
            g_med_x = group["X_Clipped"].median()
            g_med_y = group["Y_Clipped"].median()

            if g_med_x > med_x and g_med_y > med_y:
                label = "ğŸ’ ë°°ë‹¹ì„±ì¥ì£¼ (Growth+Yield)"
            elif g_med_x > med_x and g_med_y <= med_y:
                label = "ğŸ›¡ï¸ ê³ ë°°ë‹¹ ë°©ì–´ì£¼ (High Yield)"
            elif g_med_x <= med_x and g_med_y > med_y:
                label = "ğŸš€ ê³ ì„±ì¥ ê¸°ëŒ€ì£¼ (High Growth)"
            else:
                label = "ğŸ“‰ ì†Œì™¸ì£¼/ê°€ì¹˜ì£¼ (Value/Lagging)"

            cluster_info.append({"Cluster": c, "Label": label, "Count": len(group)})

        return df, pd.DataFrame(cluster_info), best_eps


class Visualizer:
    def plot(self, df, cluster_info, date_str, eps):
        plt.figure(figsize=FIG_SIZE)

        # 1. ë…¸ì´ì¦ˆ (íë¦¿í•˜ê²Œ) - ì›ë³¸ ê°’ ì‚¬ìš©í•˜ë˜ ë„ˆë¬´ í° ê±´ ì˜ë¦¼
        noise = df[df["Cluster"] == -1]
        plt.scatter(
            noise["X_Raw"],
            noise["Y_Raw"],
            c="lightgray",
            marker=".",
            s=10,
            alpha=0.1,
            label="_nolegend_",
        )

        # 2. êµ°ì§‘ ê·¸ë¦¬ê¸°
        label_map = dict(zip(cluster_info["Cluster"], cluster_info["Label"]))
        unique_labels = sorted(list(set(df["Cluster"]) - {-1}))
        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))

        for k, col in zip(unique_labels, colors):
            group = df[df["Cluster"] == k]
            label_text = label_map.get(k, f"G{k}")

            plt.scatter(
                group["X_Raw"],
                group["Y_Raw"],
                s=np.log1p(group["Marcap"]) * 3 + 50,
                color=col,
                label=f"{label_text}",
                alpha=0.8,
                edgecolors="w",
            )

            cx, cy = group["X_Raw"].median(), group["Y_Raw"].median()
            plt.text(
                cx,
                cy,
                f"G{k}",
                fontsize=12,
                fontweight="bold",
                bbox=dict(facecolor="white", alpha=0.6, edgecolor="none", pad=2),
            )

        # 3. í…ìŠ¤íŠ¸
        texts = []
        clustered_df = df[df["Cluster"] != -1]
        top_stocks = clustered_df.sort_values("Marcap", ascending=False).head(30)

        for _, row in top_stocks.iterrows():
            texts.append(
                plt.text(
                    row["X_Raw"],
                    row["Y_Raw"],
                    row["Name"],
                    fontsize=9,
                    fontweight="bold",
                )
            )

        try:
            adjust_text(texts, arrowprops=dict(arrowstyle="-", color="gray", lw=0.5))
        except:
            pass

        # 4. [í•µì‹¬] ê·¸ë˜í”„ ë²”ìœ„ ì œí•œ (Zoom In íš¨ê³¼)
        # ìƒìœ„ 1% ê°’ê¹Œì§€ë§Œ ë³´ì—¬ì£¼ë„ë¡ ì¶• ë²”ìœ„ ì„¤ì •
        x_limit = df["X_Raw"].quantile(0.99) * 1.1  # ì—¬ìœ  10%
        y_min = df["Y_Raw"].quantile(0.01) * 1.1
        y_max = df["Y_Raw"].quantile(0.99) * 1.1

        # ë§Œì•½ ë°°ë‹¹ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìµœì†Œ 10%ëŠ” ë³´ì—¬ì¤Œ
        if x_limit < 10:
            x_limit = 10

        plt.xlim(-0.5, x_limit)
        plt.ylim(y_min, y_max)

        n_clusters = len(unique_labels)
        plt.title(
            f"Market Map: Dividend vs Momentum ({n_clusters} Groups) - {date_str}",
            fontsize=16,
        )
        plt.xlabel("Dividend Yield (%)", fontsize=12)
        plt.ylabel("Momentum (Return %)", fontsize=12)
        plt.legend(loc="upper right", frameon=True, framealpha=0.9, fontsize=10)
        plt.grid(True, linestyle="--", alpha=0.3)

        out_dir = os.path.abspath(OUTPUT_FOLDER)
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)

        save_path = os.path.join(out_dir, f"dbscan_market_map_{date_str}.png")
        plt.savefig(save_path, dpi=300, bbox_inches="tight")

        cluster_info.to_csv(
            os.path.join(out_dir, f"cluster_summary_{date_str}.csv"),
            index=False,
            encoding=CSV_ENCODING,
        )
        df.to_csv(
            os.path.join(out_dir, f"cluster_details_{date_str}.csv"),
            index=False,
            encoding=CSV_ENCODING,
        )

        print(f"[ì €ì¥ ì™„ë£Œ] {save_path}")
        print("\n[êµ°ì§‘ ìš”ì•½]")
        print(cluster_info[["Label", "Count"]].to_string(index=False))
        plt.show()

def report_AutoDBScan() :
    viz = Visualizer()

    print("=" * 60)
    print(" [StockClustering] ì£¼ì‹ ì‹œì¥ ì§€ë„ (ë°°ë‹¹ vs ëª¨ë©˜í…€)")
    print("=" * 60)

    while True:
        try:
            user_input = input("\n>> ë‚ ì§œ (YYYY-MM-DD) [q:ì¢…ë£Œ]: ").strip()
            if not user_input:
                continue
            if user_input.lower() in ["q", "quit"]:
                break
            if "python" in user_input:
                continue

            df = proc.load_snapshot(user_input)
            if df.empty:
                print("ë°ì´í„° ì—†ìŒ")
                continue

            df_clustered, cluster_info, final_eps = auto_dbscan.run(df)
            viz.plot(df_clustered, cluster_info, user_input, final_eps)

        except Exception as e:
            print(f"[ì˜¤ë¥˜] {e}")
            traceback.print_exc()

if __name__ == "__main__":
    proc = DataProcessor()
    auto_dbscan = AutoDBSCAN()
    report_AutoDBScan()