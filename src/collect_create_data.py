import os
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from threading import Lock

import FinanceDataReader as fdr
import numpy as np
import pandas as pd
from tqdm import tqdm
from tqdm.auto import tqdm as tqdm_auto

tqdm_auto.pandas()
warnings.filterwarnings("ignore")


# ------------------------------------------------------------------
# ì„¤ì •íŒŒì¼(/src/config.py)ì—ì„œ List íŒŒì¼ê³¼ ë°ì´í„° íŒŒì¼ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
# -------------------------------------------------------------------
from config import DATA_FILE_LOCATION, LIST_FILE_LOCATION

DATA_DIR = Path(DATA_FILE_LOCATION).resolve()
LIST_DIR = Path(LIST_FILE_LOCATION).resolve()
STOCK_LIST_FILE = LIST_DIR / "stock_list.csv"
FINANCIALS_FILE = LIST_DIR / "financials.xlsx"
DELIST_REPORT_FILE = LIST_DIR / "stock_delist.md"

# Technical analysis constants
SQRT_252 = np.sqrt(252)
ALPHA_14 = 1 / 14
EPSILON = 1e-10

RETURN_WINDOWS = [1, 5, 20, 30, 50, 60, 100, 120, 200]
DISPARITY_WINDOWS = [5, 20, 60, 120]
DELISTING_THRESHOLD_DAYS = 10

# Adaptive worker counts based on CPU cores
CPU_COUNT = os.cpu_count() or 2
MAX_DOWNLOAD_WORKERS = min(8, max(1, CPU_COUNT // 2))
MAX_EXCEL_WORKERS = min(4, max(1, CPU_COUNT // 2))


# Utility function to clean ticker codes
def clean_ticker(ticker_series, strip_prefix=True):
    """
    Clean ticker codes to 6-digit format.

    Args:
        ticker_series: pandas Series of ticker codes
        strip_prefix: If True, removes 'a' or 'A' prefix before padding

    Returns:
        pandas Series of cleaned 6-digit ticker codes
    """
    ticker_str = ticker_series.astype(str)
    if strip_prefix:
        ticker_str = ticker_str.str.lstrip("aA")
    return ticker_str.str.zfill(6)


def data_download(start_date="2015-01-01", end_date="2024-12-31"):
    print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {DATA_DIR}")
    print("íŒŒì¼ ìƒíƒœ:")
    print(f"  âœ“ stock_list.csv: {STOCK_LIST_FILE.exists()}")
    print(f"  âœ“ financials.xlsx: {FINANCIALS_FILE.exists()}")

    if not STOCK_LIST_FILE.exists():
        raise FileNotFoundError(f"stock_list.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {STOCK_LIST_FILE}")
    if not FINANCIALS_FILE.exists():
        raise FileNotFoundError(
            f"financials.xlsxë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FINANCIALS_FILE}"
        )

    ticker_df = pd.read_csv(STOCK_LIST_FILE, encoding="cp949")
    ticker_series = clean_ticker(ticker_df.iloc[:, 0], strip_prefix=False)
    tickers = ticker_series.tolist()

    ticker_to_name = dict(zip(ticker_series, ticker_df.iloc[:, 1]))

    print(f"\nâœ“ {len(ticker_df)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ")
    print(f"âœ“ {len(tickers)}ê°œ í‹°ì»¤ ì¶”ì¶œ ì™„ë£Œ")
    print(f"ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"ìˆ˜ì§‘í•  ì¢…ëª© ìˆ˜: {len(tickers)}\n")
    print(
        f"ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •: CPU {CPU_COUNT}ì½”ì–´ â†’ ë‹¤ìš´ë¡œë“œ {MAX_DOWNLOAD_WORKERS}ê°œ, Excel {MAX_EXCEL_WORKERS}ê°œ ì›Œì»¤\n"
    )

    # ========== PARALLEL EXECUTION: Stock Downloads + Excel Loading ==========
    print("=" * 80)
    print("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 80 + "\n")

    def download_stocks_task():
        """Task 1: Download stock data from API"""

        def download_single_stock(ticker):
            try:
                stock_df = fdr.DataReader(ticker, start_date, end_date)

                if not stock_df.empty:
                    stock_df["Ticker"] = ticker
                    stock_df = stock_df.reset_index()
                    return ("success", ticker, stock_df)
                else:
                    stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")
                    return ("empty", ticker, stock_name)

            except Exception as e:
                stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")
                return ("error", ticker, stock_name, str(e)[:100])

        all_stocks = []
        failed_tickers = []
        delisted_info = []
        list_lock = Lock()

        with ThreadPoolExecutor(max_workers=MAX_DOWNLOAD_WORKERS) as executor:
            future_to_ticker = {
                executor.submit(download_single_stock, ticker): ticker
                for ticker in tickers
            }

            for future in tqdm(
                as_completed(future_to_ticker),
                total=len(tickers),
                desc="Stock Download",
                ncols=80,
                position=0,
                leave=True,
            ):
                result = future.result()

                with list_lock:
                    if result[0] == "success":
                        _, ticker, stock_df = result
                        all_stocks.append(stock_df)

                    elif result[0] == "empty":
                        _, ticker, stock_name = result
                        failed_tickers.append(ticker)
                        delisted_info.append(
                            {
                                "ì¢…ëª©ì½”ë“œ": ticker,
                                "ì¢…ëª©ëª…": stock_name,
                                "ìƒíƒœ": "ë°ì´í„°ì—†ìŒ",
                                "ë§ˆì§€ë§‰ê±°ë˜ì¼": "N/A",
                                "ì‚¬ìœ ": "ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                            }
                        )

                    else:
                        _, ticker, stock_name, error_msg = result
                        failed_tickers.append(ticker)
                        delisted_info.append(
                            {
                                "ì¢…ëª©ì½”ë“œ": ticker,
                                "ì¢…ëª©ëª…": stock_name,
                                "ìƒíƒœ": "ì˜¤ë¥˜ë°œìƒ",
                                "ë§ˆì§€ë§‰ê±°ë˜ì¼": "N/A",
                                "ì‚¬ìœ ": error_msg,
                            }
                        )

        print(
            f"\nì£¼ì‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(all_stocks)}ê°œ ì„±ê³µ, {len(failed_tickers)}ê°œ ì‹¤íŒ¨\n"
        )

        return all_stocks, failed_tickers, delisted_info

    def load_excel_task():
        """Task 2: Load financial data from Excel"""

        # Pre-compute ticker set once (shared by all functions)
        tickers_set = set(tickers)

        def load_financial_sheet_filtered(sheet_name):
            # Create a new ExcelFile instance for each thread (calamine is not thread-safe)
            try:
                excel_file_obj = pd.ExcelFile(FINANCIALS_FILE, engine="calamine")
            except ImportError:
                excel_file_obj = pd.ExcelFile(FINANCIALS_FILE, engine="openpyxl")

            # Single-pass: Load sheet and filter columns after
            df_raw = excel_file_obj.parse(sheet_name, header=None)

            all_tickers_clean = clean_ticker(df_raw.iloc[0, 1:], strip_prefix=True)
            dates = pd.to_datetime(df_raw.iloc[3:, 0])

            mask = all_tickers_clean.isin(tickers_set)
            cols_to_keep = [i + 1 for i, keep in enumerate(mask) if keep]
            tickers_kept = all_tickers_clean[mask].tolist()

            values = df_raw.iloc[3:, cols_to_keep]

            df = pd.DataFrame(values.values, index=dates, columns=tickers_kept)
            df = df.apply(pd.to_numeric, errors="coerce")
            df = df.sort_index()

            return sheet_name, df

        def process_metric_rows(filtered_rows, dates):
            """Helper to process metric rows into DataFrame with dates as index"""
            if filtered_rows.empty:
                return pd.DataFrame(index=dates)

            metric_data = filtered_rows.set_index("ticker_clean").iloc[:, 3:-2].T

            # Handle length mismatch between dates and transposed data
            # This can happen if the Excel file has trailing columns
            min_len = min(len(metric_data), len(dates))
            metric_data = metric_data.iloc[:min_len]
            metric_data.index = dates[:min_len]

            return metric_data.apply(pd.to_numeric, errors="coerce").sort_index()

        def load_raw_sheet_metrics():
            # Create a new ExcelFile instance for each thread (calamine is not thread-safe)
            try:
                excel_file_obj = pd.ExcelFile(FINANCIALS_FILE, engine="calamine")
            except ImportError:
                excel_file_obj = pd.ExcelFile(FINANCIALS_FILE, engine="openpyxl")

            df_raw = excel_file_obj.parse("RAW", header=None)

            # Clean ticker column using modular function
            ticker_col = clean_ticker(df_raw.iloc[:, 0], strip_prefix=True)
            metric_col = df_raw.iloc[:, 2].astype(str)

            # Calculate dates once
            dates = pd.to_datetime(df_raw.iloc[0, 3:])

            # Filter rows early to reduce memory
            ticker_mask = ticker_col.isin(tickers_set)
            df_filtered = df_raw[ticker_mask].copy()

            if df_filtered.empty:
                return pd.DataFrame(index=dates), pd.DataFrame(index=dates)

            df_filtered["ticker_clean"] = ticker_col[ticker_mask]
            df_filtered["metric"] = metric_col[ticker_mask]

            # Use modular function to process both metrics
            mask_revenue = df_filtered["metric"].str.contains("ë§¤ì¶œì•¡", na=False)
            mask_op_profit = df_filtered["metric"].str.contains("ì˜ì—…ì´ìµ", na=False)

            df_revenue = process_metric_rows(df_filtered[mask_revenue], dates)
            df_op_profit = process_metric_rows(df_filtered[mask_op_profit], dates)

            return df_revenue, df_op_profit

        financial_sheets = ["BPS", "DPS", "EPS", "ë°°ë‹¹ìˆ˜ìµë¥ "]
        sheet_results = {}
        total_tasks = len(financial_sheets) + 1  # +1 for RAW_METRICS

        with ThreadPoolExecutor(max_workers=MAX_EXCEL_WORKERS) as executor:
            futures = {
                executor.submit(load_financial_sheet_filtered, sheet): sheet
                for sheet in financial_sheets
            }
            futures[executor.submit(load_raw_sheet_metrics)] = "RAW_METRICS"

            with tqdm(
                total=total_tasks,
                desc="Financial Data",
                ncols=80,
                position=1,
                leave=True,
            ) as pbar:
                for future in as_completed(futures):
                    task_name = futures[future]
                    if task_name == "RAW_METRICS":
                        df_revenue, df_op_profit = future.result()
                    else:
                        sheet_name, df = future.result()
                        sheet_results[sheet_name] = df
                    pbar.update(1)

        df_bps = sheet_results["BPS"]
        df_dps = sheet_results["DPS"]
        df_eps = sheet_results["EPS"]
        df_div_yield = sheet_results["ë°°ë‹¹ìˆ˜ìµë¥ "]

        print("\nì¬ë¬´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(
            f"  BPS/DPS/EPS/ë°°ë‹¹ìˆ˜ìµë¥ : {df_bps.shape[0]} dates x {df_bps.shape[1]} tickers"
        )
        print(
            f"  ë§¤ì¶œì•¡/ì˜ì—…ì´ìµ: {df_revenue.shape[0]} dates x {df_revenue.shape[1]} tickers\n"
        )

        return df_bps, df_dps, df_eps, df_div_yield, df_revenue, df_op_profit

    # Run both tasks in parallel
    with ThreadPoolExecutor(max_workers=2) as main_executor:
        stock_future = main_executor.submit(download_stocks_task)
        excel_future = main_executor.submit(load_excel_task)

        all_stocks, failed_tickers, delisted_info = stock_future.result()
        df_bps, df_dps, df_eps, df_div_yield, df_revenue, df_op_profit = (
            excel_future.result()
        )

    print("=" * 80)
    print("ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print("=" * 80 + "\n")

    # ========== Process stock data ==========
    if not all_stocks:
        raise ValueError("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    df_all = pd.concat(all_stocks, ignore_index=True, copy=False)
    df_all = df_all.sort_values(["Ticker", "Date"]).reset_index(drop=True)

    print(f"ê²°í•©ëœ DataFrame í¬ê¸°: {df_all.shape}")
    print(f"ë‚ ì§œ ë²”ìœ„: {df_all['Date'].min()} ~ {df_all['Date'].max()}")
    print(f"ê³ ìœ  ì¢…ëª© ìˆ˜: {df_all['Ticker'].nunique()}\n")

    df_all["ì¢…ëª©ëª…"] = df_all["Ticker"].map(ticker_to_name).astype("category")

    cols = df_all.columns.tolist()
    cols.remove("ì¢…ëª©ëª…")
    cols.insert(1, "ì¢…ëª©ëª…")
    df_all = df_all[cols]

    print(f"DataFrame í¬ê¸°: {df_all.shape}\n")

    def calculate_all_features(group):
        for window in RETURN_WINDOWS:
            group[f"Return_{window}d"] = group["Close"].pct_change(periods=window) * 100

        group["vol_20"] = (
            group["Close"].pct_change().rolling(window=20, min_periods=20).std()
        )
        group["vol_60"] = (
            group["Close"].pct_change().rolling(window=60, min_periods=60).std()
        )
        group["vol_60_sqrt252"] = group["vol_60"] * SQRT_252
        group["log_vol"] = np.log(group["Volume"] + 1)
        log_vol_mean_60 = group["log_vol"].rolling(window=60, min_periods=60).mean()
        group["vol_ratio_60"] = group["log_vol"] - log_vol_mean_60
        group["avg_log_vol_ratio_60"] = (
            group["vol_ratio_60"].rolling(window=60, min_periods=60).mean()
        )
        group["std_log_vol_ratio_60"] = (
            group["vol_ratio_60"].rolling(window=60, min_periods=60).std()
        )

        delta = group["Close"].diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        avg_gain = gain.ewm(alpha=ALPHA_14, adjust=False, min_periods=14).mean()
        avg_loss = loss.ewm(alpha=ALPHA_14, adjust=False, min_periods=14).mean()
        rs = avg_gain / (avg_loss + EPSILON)
        group["RSI_14"] = 100 - (100 / (1 + rs))
        group["RSI_14_60avg"] = (
            group["RSI_14"].rolling(window=60, min_periods=60).mean()
        )

        typical_price = (group["High"] + group["Low"] + group["Close"]) / 3
        money_flow = typical_price * group["Volume"]
        positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0)
        negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0)
        positive_mf = positive_flow.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        negative_mf = negative_flow.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        mfi_ratio = positive_mf / (negative_mf + EPSILON)
        group["MFI_14"] = 100 - (100 / (1 + mfi_ratio))

        high_low = group["High"] - group["Low"]
        high_close = np.abs(group["High"] - group["Close"].shift(1))
        low_close = np.abs(group["Low"] - group["Close"].shift(1))
        true_range = pd.Series(
            np.maximum.reduce([high_low, high_close, low_close]), index=group.index
        )
        group["ATR_14"] = true_range.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        group["NATR_14"] = (group["ATR_14"] / group["Close"]) * 100

        high_diff = group["High"].diff()
        low_diff = -group["Low"].diff()
        plus_dm = high_diff.where((high_diff > low_diff) & (high_diff > 0), 0)
        minus_dm = low_diff.where((low_diff > high_diff) & (low_diff > 0), 0)
        smoothed_tr = true_range.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        smoothed_plus_dm = plus_dm.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        smoothed_minus_dm = minus_dm.ewm(
            alpha=ALPHA_14, adjust=False, min_periods=14
        ).mean()
        plus_di = 100 * (smoothed_plus_dm / (smoothed_tr + EPSILON))
        minus_di = 100 * (smoothed_minus_dm / (smoothed_tr + EPSILON))
        dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + EPSILON)
        group["ADX_14"] = dx.ewm(alpha=ALPHA_14, adjust=False, min_periods=14).mean()

        for window in DISPARITY_WINDOWS:
            ma = group["Close"].rolling(window=window, min_periods=window).mean()
            group[f"Disparity_{window}d"] = ((group["Close"] - ma) / ma) * 100

        daily_returns = group["Close"].pct_change()
        group["Mean_60d"] = daily_returns.rolling(window=60, min_periods=60).mean()
        group["Median_60d"] = daily_returns.rolling(window=60, min_periods=60).median()
        group["Std_60d"] = daily_returns.rolling(window=60, min_periods=60).std()
        group["Sharpe_60d"] = group["Mean_60d"] / (group["Std_60d"] + EPSILON)

        mean_252 = daily_returns.rolling(window=252, min_periods=252).mean()
        std_252 = daily_returns.rolling(window=252, min_periods=252).std()
        group["Sharpe_252d"] = mean_252 / (std_252 + EPSILON)

        downside_returns = daily_returns.where(daily_returns < 0, 0)
        downside_std_60 = downside_returns.rolling(window=60, min_periods=60).std()
        group["Sortino_60d"] = group["Mean_60d"] / (downside_std_60 + EPSILON)

        downside_std_252 = downside_returns.rolling(window=252, min_periods=252).std()
        group["Sortino_252d"] = mean_252 / (downside_std_252 + EPSILON)

        group["Skewness_60d"] = daily_returns.rolling(window=60, min_periods=60).skew()
        group["Zscore_60d"] = (
            group["Close"] - group["Close"].rolling(window=60, min_periods=60).mean()
        ) / (group["Close"].rolling(window=60, min_periods=60).std() + EPSILON)

        risk_cols = [
            "Mean_60d",
            "Median_60d",
            "Std_60d",
            "Sharpe_60d",
            "Sharpe_252d",
            "Sortino_60d",
            "Sortino_252d",
            "Skewness_60d",
            "Zscore_60d",
        ]
        group[risk_cols] = group[risk_cols].replace([np.inf, -np.inf], np.nan)

        return group

    try:
        df_all = df_all.groupby("Ticker", group_keys=False).progress_apply(
            calculate_all_features
        )
    except AttributeError:
        df_all = df_all.groupby("Ticker", group_keys=False).apply(
            calculate_all_features
        )

    print(f"\nDataFrame í¬ê¸°: {df_all.shape}")
    print(f"ì´ ì»¬ëŸ¼ ìˆ˜: {len(df_all.columns)}\n")

    # ========== Financial data already loaded in parallel above ==========

    print("Applying lags...\n")

    def apply_financial_lag(df_financial):
        df_lagged = df_financial.copy()
        df_lagged.index = df_lagged.index + pd.DateOffset(months=3)
        return df_lagged

    def apply_quarterly_lag(df_quarterly):
        df_lagged = df_quarterly.copy()

        new_index = []
        for date in df_lagged.index:
            month = date.month
            year = date.year

            if month == 3:
                new_date = pd.Timestamp(year=year, month=4, day=1)
            elif month == 6:
                new_date = pd.Timestamp(year=year, month=7, day=1)
            elif month == 9:
                new_date = pd.Timestamp(year=year, month=10, day=1)
            elif month == 12:
                new_date = pd.Timestamp(year=year + 1, month=1, day=1)
            else:
                new_date = date

            new_index.append(new_date)

        df_lagged.index = pd.DatetimeIndex(new_index)
        return df_lagged

    df_bps_lagged = apply_financial_lag(df_bps)
    df_dps_lagged = apply_financial_lag(df_dps)
    df_eps_lagged = apply_financial_lag(df_eps)
    df_div_yield_lagged = apply_financial_lag(df_div_yield)

    df_revenue_lagged = apply_quarterly_lag(df_revenue)
    df_op_profit_lagged = apply_quarterly_lag(df_op_profit)

    print("âœ“ Lags applied:")
    print("  - Daily data (BPS, DPS, EPS, dividend yield): 3 months lag")
    print("  - Quarterly data (revenue, operating profit): next quarter start")
    print("    Â· Q1 (Mar 31) â†’ Apr 1")
    print("    Â· Q2 (Jun 30) â†’ Jul 1")
    print("    Â· Q3 (Sep 30) â†’ Oct 1")
    print("    Â· Q4 (Dec 31) â†’ Jan 1 (next year)\n")

    print("Merging financial data and calculating derived metrics...\n")

    def merge_financial_data(group):
        ticker = group.name

        group = group.reset_index()

        if "Date" not in group.columns and "index" in group.columns:
            group = group.rename(columns={"index": "Date"})

        group["Date"] = pd.to_datetime(group["Date"])
        group = group.sort_values("Date")

        mapping = {
            "BPS": df_bps_lagged,
            "DPS": df_dps_lagged,
            "EPS": df_eps_lagged,
            "ë°°ë‹¹ìˆ˜ìµë¥ ": df_div_yield_lagged,
            "ë§¤ì¶œì•¡": df_revenue_lagged,
            "ì˜ì—…ì´ìµ": df_op_profit_lagged,
        }

        for col_name, df_source in mapping.items():
            if ticker in df_source.columns:
                f_data = df_source[ticker].dropna().to_frame(name=col_name)
                f_data = f_data.reset_index()
                f_data.columns = ["Date", col_name]
                f_data["Date"] = pd.to_datetime(f_data["Date"])
                f_data = f_data.sort_values("Date")

                group = pd.merge_asof(group, f_data, on="Date", direction="backward")
            else:
                group[col_name] = np.nan

        group["PER"] = group["Close"] / group["EPS"]
        group["PBR"] = group["Close"] / group["BPS"]
        group["ROE"] = group["PBR"] / group["PER"]
        group["ë°°ë‹¹ì„±í–¥"] = (group["DPS"] / group["EPS"]) * 100

        group["ROE_YoY"] = (group["ROE"] / group["ROE"].shift(252) - 1) * 100
        group["EPS_YoY"] = (group["EPS"] / group["EPS"].shift(252) - 1) * 100
        group["ì˜ì—…ì´ìµ_YoY"] = (
            group["ì˜ì—…ì´ìµ"] / group["ì˜ì—…ì´ìµ"].shift(252) - 1
        ) * 100
        group["ë§¤ì¶œì•¡_YoY"] = (group["ë§¤ì¶œì•¡"] / group["ë§¤ì¶œì•¡"].shift(252) - 1) * 100

        metrics = [
            "PER",
            "PBR",
            "ROE",
            "ë°°ë‹¹ì„±í–¥",
            "ROE_YoY",
            "EPS_YoY",
            "ì˜ì—…ì´ìµ_YoY",
            "ë§¤ì¶œì•¡_YoY",
        ]
        group[metrics] = group[metrics].replace([np.inf, -np.inf], np.nan)

        group["Ticker"] = ticker

        return group.set_index(["Date", "Ticker"])

    try:
        df_all = df_all.groupby("Ticker", group_keys=False).progress_apply(
            merge_financial_data
        )
    except AttributeError:
        df_all = df_all.groupby("Ticker", group_keys=False).apply(merge_financial_data)

    print(f"\nìµœì¢… DataFrame í¬ê¸°: {df_all.shape}")
    print(f"ì´ ì»¬ëŸ¼ ìˆ˜: {len(df_all.columns)}\n")

    # Rotate data directories: data -> data_old (keep only 2 versions)
    output_dir = DATA_DIR
    old_dir = DATA_DIR.parent / "data_old"

    if output_dir.exists():
        # If data_old exists, delete it first
        if old_dir.exists():
            import shutil

            shutil.rmtree(old_dir)
            print(f"Removed old backup: {old_dir}")

        # Rename current data to data_old
        output_dir.rename(old_dir)
        print(f"Backed up current data: {output_dir} -> {old_dir}")

    # Create fresh data directory
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Created new data directory: {output_dir}\n")

    all_tickers = df_all.index.get_level_values("Ticker").unique()

    print(f"ê°œë³„ ì¢…ëª© íŒŒì¼ ì €ì¥ ì¤‘: {output_dir}\n")

    stock_metadata = {}
    col_count = 0

    for ticker in tqdm(all_tickers, desc="Saving Files", ncols=80):
        stock_df = df_all.xs(ticker, level="Ticker").copy()

        cols_to_drop = ["Ticker", "index", "level_0"]
        stock_df = stock_df.drop(
            columns=[c for c in cols_to_drop if c in stock_df.columns]
        )

        stock_df = stock_df.sort_index()

        if col_count == 0:
            col_count = stock_df.shape[1]

        stock_metadata[ticker] = {
            "last_date": stock_df.index.max(),
            "last_close": stock_df.loc[stock_df.index.max(), "Close"],
        }

        stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")

        filename = f"{ticker}_{stock_name}.parquet"
        filepath = output_dir / filename

        stock_df.to_parquet(
            filepath, engine="pyarrow", compression="snappy", index=True
        )

    unique_ticker_count = len(all_tickers)

    print("\nìƒì¥íì§€ ì¢…ëª© ê²€ì‚¬ ì¤‘...\n")

    end_date_dt = pd.to_datetime(end_date)
    delisting_threshold = end_date_dt - pd.Timedelta(days=DELISTING_THRESHOLD_DAYS)

    for ticker, metadata in tqdm(
        stock_metadata.items(), desc="Delisting Check", ncols=80
    ):
        last_date = metadata["last_date"]
        last_close = metadata["last_close"]

        stock_name = ticker_to_name.get(ticker, "ì•Œìˆ˜ì—†ìŒ")

        is_delisted = False
        delisting_reason = ""

        if pd.isna(last_close) or last_close == 0:
            is_delisted = True
            delisting_reason = "ì¢…ê°€ê°€ 0 ë˜ëŠ” NaN"
        elif last_date < delisting_threshold:
            is_delisted = True
            delisting_reason = (
                f"ê±°ë˜ ì¤‘ë‹¨ ({last_date.strftime('%Y-%m-%d')}ì— ë§ˆì§€ë§‰ ê±°ë˜)"
            )

        if is_delisted:
            delisted_info.append(
                {
                    "ì¢…ëª©ì½”ë“œ": ticker,
                    "ì¢…ëª©ëª…": stock_name,
                    "ìƒíƒœ": "ìƒì¥íì§€",
                    "ë§ˆì§€ë§‰ê±°ë˜ì¼": last_date.strftime("%Y-%m-%d"),
                    "ì‚¬ìœ ": delisting_reason,
                }
            )

    print(f"\n{'=' * 80}")
    print("ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'=' * 80}")
    print(f"âœ“ {unique_ticker_count}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    print(f"âœ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"âœ“ íŒŒì¼ë‹¹ ì»¬ëŸ¼ ìˆ˜: {col_count}ê°œ")
    print(f"{'=' * 80}\n")

    if delisted_info:
        delisted_md_path = DELIST_REPORT_FILE

        delisted_count = 0
        error_count = 0
        for d in delisted_info:
            if d["ìƒíƒœ"] == "ìƒì¥íì§€":
                delisted_count += 1
            else:
                error_count += 1
        active_count = unique_ticker_count - delisted_count

        with open(delisted_md_path, "w", encoding="utf-8") as f:
            f.write("# ìƒì¥íì§€ ë° ë°ì´í„° ì˜¤ë¥˜ ì¢…ëª© ëª©ë¡\n\n")
            f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}\n\n")
            f.write("## ğŸ“Š ìš”ì•½\n\n")
            f.write(f"- ì´ ë¶„ì„ ì¢…ëª© ìˆ˜: {unique_ticker_count}ê°œ\n")
            f.write(f"- ì •ìƒ ê±°ë˜ ì¢…ëª©: {active_count}ê°œ\n")
            f.write(f"- ìƒì¥íì§€ ì¢…ëª©: {delisted_count}ê°œ\n")
            f.write(f"- ë°ì´í„° ì˜¤ë¥˜ ì¢…ëª©: {error_count}ê°œ\n\n")
            f.write("---\n\n")
            f.write("## ğŸ” ìƒì„¸ ëª©ë¡\n\n")
            f.write("| ì¢…ëª©ì½”ë“œ | ì¢…ëª©ëª… | ìƒíƒœ | ë§ˆì§€ë§‰ê±°ë˜ì¼ | ì‚¬ìœ  |\n")
            f.write("|---------|--------|------|-------------|------|\n")
            for info in delisted_info:
                f.write(
                    f"| {info['ì¢…ëª©ì½”ë“œ']} | {info['ì¢…ëª©ëª…']} | {info['ìƒíƒœ']} | {info['ë§ˆì§€ë§‰ê±°ë˜ì¼']} | {info['ì‚¬ìœ ']} |\n"
                )
            f.write("\n---\n\n")
            f.write("## ğŸ“ ì°¸ê³ ì‚¬í•­\n\n")
            f.write(
                f"- ì´ ë¦¬ìŠ¤íŠ¸ëŠ” {start_date}ë¶€í„° {end_date}ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            )
            f.write(
                f"- ë§ˆì§€ë§‰ ê±°ë˜ì¼ì´ {delisting_threshold.strftime('%Y-%m-%d')} ì´ì „ì¸ ì¢…ëª©ì€ ìƒì¥íì§€ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n"
            )
            f.write("- ì¢…ê°€ê°€ 0ì´ê±°ë‚˜ NaNì¸ ì¢…ëª©ì€ ìƒì¥íì§€ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n")
            f.write("- **ìƒì¥íì§€ ì¢…ëª©ì˜ ë°ì´í„°ëŠ” ë§ˆì§€ë§‰ ê±°ë˜ì¼ê¹Œì§€ ë³´ì¡´ë©ë‹ˆë‹¤.**\n")

        print(f"\nâœ“ ìƒì¥íì§€ ë° ì˜¤ë¥˜ ì¢…ëª© ì •ë³´ ì €ì¥: {delisted_md_path}")
        print(f"  - ì •ìƒ ê±°ë˜: {active_count}ê°œ")
        print(f"  - ìƒì¥íì§€: {delisted_count}ê°œ")
        print(f"  - ë°ì´í„° ì˜¤ë¥˜: {error_count}ê°œ\n")

    return df_all


def data_load():
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {DATA_DIR}")
    print("ë°ì´í„° í´ë” í™•ì¸ ì¤‘...\n")

    # Simply use DATA_DIR without any search logic
    if not DATA_DIR.exists():
        print(f"\nì˜¤ë¥˜: ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR.absolute()}")
        print("data_download()ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ë¨¼ì € ìˆ˜ì§‘í•˜ì„¸ìš”.")
        return None

    parquet_files = list(DATA_DIR.glob("*.parquet"))

    if not parquet_files:
        print(f"\nì˜¤ë¥˜: '{DATA_DIR.name}' í´ë”ëŠ” ì¡´ì¬í•˜ì§€ë§Œ parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    print(f"\në°ì´í„° í´ë”: {DATA_DIR.absolute()}")
    print(f"{len(parquet_files)}ê°œ parquet íŒŒì¼ ë°œê²¬")

    num_files = len(parquet_files)
    max_parquet_workers = min(CPU_COUNT, 16, num_files)
    print(f"ë³‘ë ¬ ë¡œë”©: {max_parquet_workers}ê°œ ì›Œì»¤\n")

    def load_single_parquet(file_path):
        """Load a single parquet file and add ticker"""
        try:
            stock_df = pd.read_parquet(file_path, engine="pyarrow")

            # Extract ticker from filename (format: TICKER_NAME.parquet)
            ticker_code = file_path.stem.split("_")[0]
            stock_df["Ticker"] = ticker_code

            return ("success", stock_df, None)
        except Exception as e:
            return ("error", None, (file_path.name, str(e)))

    all_stocks = []
    failed_files = []

    # Parallel loading with ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=max_parquet_workers) as executor:
        futures = {executor.submit(load_single_parquet, fp): fp for fp in parquet_files}

        for future in tqdm(
            as_completed(futures),
            total=len(parquet_files),
            desc="Loading Parquet Files",
            ncols=80,
        ):
            status, stock_df, error_info = future.result()

            if status == "success":
                all_stocks.append(stock_df)
            else:
                failed_files.append(error_info)

    if not all_stocks:
        print("\nì˜¤ë¥˜: ì½ì„ ìˆ˜ ìˆëŠ” parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return None

    print(f"\në³‘í•© ì¤‘ ({len(all_stocks)}ê°œ íŒŒì¼)...")
    df_all = pd.concat(all_stocks, ignore_index=False)
    df_all.index = pd.to_datetime(df_all.index)
    df_all.index.name = "Date"
    df_all = df_all.set_index("Ticker", append=True)
    df_all = df_all.sort_index()

    print(f"DataFrame ë¡œë“œ ì™„ë£Œ: {len(df_all):,}í–‰")

    if failed_files:
        print(f"\nê²½ê³ : ì½ì§€ ëª»í•œ íŒŒì¼ {len(failed_files)}ê°œ")
        for filename, error in failed_files[:3]:
            print(f"  {filename}: {error[:50]}")
        if len(failed_files) > 3:
            print(f"  ... ì™¸ {len(failed_files) - 3}ê°œ")

    print(f"\n{'=' * 80}")
    print(f"Index: {df_all.index.names} | Columns: {len(df_all.columns)}")
    print(f"{'=' * 80}\n")

    return df_all


def data_query(df):
    if df is None:
        print("âŒ ì˜¤ë¥˜: DataFrameì´ Noneì…ë‹ˆë‹¤. ë¨¼ì € data_load()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None

    idx = pd.IndexSlice

    print("\n" + "=" * 80)
    print("ë°ì´í„° ì¿¼ë¦¬")
    print("=" * 80)

    start_date_input = input("\nì‹œì‘ ë‚ ì§œ (YYYY-MM-DD) [ì—”í„°=ì „ì²´ ì‹œì‘ì¼]: ").strip()
    if start_date_input == "":
        start_date = df.index.get_level_values("Date").min()
        print(f"â†’ ì‹œì‘ ë‚ ì§œ: {start_date.strftime('%Y-%m-%d')} (ì „ì²´ ì‹œì‘ì¼)")
    else:
        start_date = start_date_input
        print(f"â†’ ì‹œì‘ ë‚ ì§œ: {start_date}")

    end_date_input = input("ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD) [ì—”í„°=ì „ì²´ ì¢…ë£Œì¼]: ").strip()
    if end_date_input == "":
        end_date = df.index.get_level_values("Date").max()
        print(f"â†’ ì¢…ë£Œ ë‚ ì§œ: {end_date.strftime('%Y-%m-%d')} (ì „ì²´ ì¢…ë£Œì¼)")
    else:
        end_date = end_date_input
        print(f"â†’ ì¢…ë£Œ ë‚ ì§œ: {end_date}")

    tickers_input = input(
        "\nì°¾ì„ í‹°ì»¤ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 005930,000660) [ì—”í„°=ì „ì²´]: "
    ).strip()
    if tickers_input == "":
        tickers_to_find = []
        print("â†’ í‹°ì»¤: ì „ì²´")
    else:
        tickers_to_find = [t.strip() for t in tickers_input.split(",")]
        print(f"â†’ í‹°ì»¤: {tickers_to_find}")

    columns_input = input(
        "ì°¾ì„ ì»¬ëŸ¼ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: Close,Volume,PER) [ì—”í„°=ì „ì²´]: "
    ).strip()
    if columns_input == "":
        columns_to_find = []
        print("â†’ ì»¬ëŸ¼: ì „ì²´")
    else:
        columns_to_find = [c.strip() for c in columns_input.split(",")]
        print(f"â†’ ì»¬ëŸ¼: {columns_to_find}")

    print("\n" + "=" * 80)
    print("ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
    print("=" * 80 + "\n")

    if tickers_to_find:
        df_final = df.loc[idx[start_date:end_date, tickers_to_find], :]
    else:
        df_final = df.loc[idx[start_date:end_date, :], :]

    if columns_to_find:
        available_cols = [c for c in columns_to_find if c in df_final.columns]
        missing_cols = [c for c in columns_to_find if c not in df_final.columns]

        if missing_cols:
            print(f"âš ï¸  ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼: {missing_cols}")

        if available_cols:
            df_final = df_final[available_cols]
        else:
            print("âŒ ì˜¤ë¥˜: ìœ íš¨í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return None

    print(f"\n{'=' * 80}")
    print(f"ê²€ìƒ‰ ê²°ê³¼ ({start_date} ~ {end_date})")
    print(f"{'=' * 80}")
    print(f"ë°œê²¬ëœ í‹°ì»¤: {df_final.index.get_level_values('Ticker').unique().tolist()}")
    print(f"ìœ ì§€ëœ ì»¬ëŸ¼: {df_final.columns.tolist()}")
    print(f"ì´ í–‰ ìˆ˜: {len(df_final):,}ê°œ")
    print(f"{'=' * 80}\n")

    print("ì²˜ìŒ 10í–‰:")
    print(df_final.head(10))
    print("\në§ˆì§€ë§‰ 10í–‰:")
    print(df_final.tail(10))

    return df_final
